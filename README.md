#  Yet Another Text-to-Text Generation Model using Character-based Recurrent Neural Network (RNN)

# Abstract
Text generation is a basic and very important task of Natural Language Processing (NLP). It is used in variety of NLP application like machine translation, text summarization and in conversational chatbots. But there are many problems in generating natural sentences. Recently there are many developments in recurrent neural network (RNN) especially Long-Short Term Memory is widely used in such scenarios where text generation is required. This paper will provide LSTM-based character level language model implemented using wikipedia dataset to generate text. Adam optimizer algorithm is used to optimize the model. BERTScore and BLEU is used to evaluate the model. Results shows the models outperforms the state of the art models with same settings, including logical models and statistical models.

# Keywords: text generation, RNN , LSTM, 
